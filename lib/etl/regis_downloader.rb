# -*- encoding : utf-8 -*-

require 'mechanize'

module Etl
  class RegisDownloader

    SOURCE_ROOT_URL = "http://slovak.statistics.sk"
    DOCUMENT_URL = "https://slovak.statistics.sk/wps/portal/ext/Databases/register_organizacii/"
    DEFAULT_URL_IDENTIFIER = "ORGANISATION_ID"

    attr_reader :agent, :page

    def initialize
      initialize_agent
    end

    # download all organisation by ids
    def download_organisations_by_ids(document_ids)
      default_url = default_organisation_url
      total_count = document_ids.count
      document_ids.each_with_index do |document_id, index|
        puts "#{index} / #{total_count}" if index % 1000 == 0

        url = prepare_organisation_url(default_url, document_id)
        source_url = SOURCE_ROOT_URL + url
        html = go_to_organisation_by_url(url)

        extraction = Etl::RegisExtraction.new(html, document_id, source_url)
        extraction.perform
      end
    end

    # update all organisations in ids
    def update_organisations_by_ids(from_document_id, to_document_id)
      Staging::StaRegisMain.where("doc_id >= ? AND doc_id < ?", from_document_id, to_document_id).select([:doc_id, :source_url]).find_each do |organisation|
        html = go_to_organisation_by_url(organisation.source_url)
        extraction = Etl::RegisUpdate.new(html, organisation.doc_id, organisation.source_url)
        extraction.perform
      end
    end

    def download_new_organisations(buffer = 20)
      to_page = count_pages
      from_page = to_page - buffer
      download_organisations_from_pages(from_page, to_page)
    end

    # download organisations from to page and return all missing items
    def download_organisations_from_pages(from_page = nil, to_page = nil)
      from_page ||= 1
      to_page ||= count_pages

      all_items = []
      default_url = default_page_url

      (from_page..to_page).each do |page_number|
        html = go_to_page(default_url, page_number)
        all_items += parse_page(html)
      end

      available_ids = all_items.map { |item| item[:document_id] }.sort
      actual_ids = Staging::StaRegisMain.where(doc_id: available_ids).select(:doc_id).order(:doc_id).map(&:doc_id)
      missing_ids = available_ids - actual_ids

      all_items.each do |item|
        if missing_ids.include? item[:document_id]
          source_url = SOURCE_ROOT_URL + item[:url]
          html = go_to_organisation_by_url(item[:url])
          extraction = Etl::RegisExtraction.new(html, item[:document_id], source_url)
          extraction.perform
        end
      end

      # return all missing ids
      missing_ids
    end

    # update all items with new urls
    def update_all_source_urls
      default_url = nil
      total_count = Staging::StaRegisMain.count
      index = 0
      Staging::StaRegisMain.order(:doc_id).find_each do |organisation|
        # refresh default url
        default_url = default_organisation_url if index % 10000 == 0
        puts "#{index} / #{total_count}" if index % 1000 == 0
        organisation.update_attribute(:source_url, SOURCE_ROOT_URL + prepare_organisation_url(default_url, organisation.doc_id))
        index += 1
      end
    end

    # return hash as urls generated by ids
    def get_url_for_ids(document_ids)
      default_url = default_organisation_url
      document_urls = {}
      document_ids.each do |document_id|
        document_urls[document_id] = SOURCE_ROOT_URL + prepare_organisation_url(default_url, document_id)
      end
      document_urls
    end

    private

    def initialize_agent
      @agent = Mechanize.new
      @page = agent.get(DOCUMENT_URL)
    end

    def count_pages
      search_form = page.form('searchIcoForm')
      search_results = agent.submit(search_form)

      url = search_results.at("span.selectPage a").first[1]
      url = url.gsub("p=pageSize=25", "p=pageSize=1000").gsub("p=page=2", "p=page=1")

      search_results = agent.get(url)
      search_results.at("span.selectPage:last a").text.to_i
    end

    def default_page_url
      search_form = page.form('searchIcoForm')
      search_results = agent.submit(search_form)

      url = search_results.at("span.selectPage a").first[1]
      url.gsub("p=pageSize=25", "p=pageSize=1000").gsub("p=page=2", "p=page=1")
    end

    def default_organisation_url
      search_form = page.form('searchIcoForm')
      search_results = agent.submit search_form
      doc = Nokogiri::HTML(search_results.body)
      items = doc.css(".publications-list tr.listTableRow0")
      if items.any?
        tr = items[0]
        tds = tr.css("td")
        url = tds[1].css("a").first.attributes["href"].text
        document_id = url.match(/(.*)=roId=(.*)\/p=(.*)/)[2]
        url.gsub("=roId=#{document_id}", "=roId=#{DEFAULT_URL_IDENTIFIER}")
      else
        ""
      end
    end

    def go_to_organisation_by_url(url)
      page = agent.get url
      page.body
    end

    def prepare_organisation_url(default_url, id)
      default_url.gsub("=roId=#{DEFAULT_URL_IDENTIFIER}", "=roId=#{id}")
    end

    def go_to_page(url, page_number)
      page = agent.get url.gsub("p=page=1", "p=page=#{page_number}")
      page.body
    end

    def parse_page(html)
      doc = Nokogiri::HTML(html)
      items = doc.css(".publications-list tr.listTableRow0") + doc.css(".publications-list tr.listTableRow1")
      ids = []
      items.each do |tr|
        tds = tr.css("td")
        url = tds[1].css("a").first.attributes["href"].text
        document_id = url.match(/(.*)=roId=(.*)\/p=(.*)/)[2].to_i
        ids << {:document_id => document_id, :url => url}
      end

      ids
    end

  end
end
